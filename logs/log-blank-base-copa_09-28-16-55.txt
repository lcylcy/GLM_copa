[2021-09-28 16:55:31,308] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-09-28 16:55:32,557] [INFO] [runner.py:360:main] cmd = /dataset/lichunyou/conda_env/miniconda3/envs/p7/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=59147 finetune_glm.py --deepspeed --deepspeed_config config_tasks/config_blocklm_10B.json --finetune --cloze-eval --experiment-name blank-base-copa_09-28-16-55 --task COPA --data-dir /dataset/lichunyou/GLM/GLM_copa/dataset/COPA --save /dataset/lichunyou/GLM/GLM_copa/copa_model --seq-length 256 --checkpoint-activations --eval-batch-size 4 --save-epoch 100000 --num-workers 1 --no-load-optim --no-load-lr-scheduler --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --pattern-id 0 --fp16 --model-parallel-size 1 --epochs -1 --overwrite
[2021-09-28 16:55:33,265] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-09-28 16:55:33,265] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-09-28 16:55:33,265] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-09-28 16:55:33,265] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}
[2021-09-28 16:55:33,265] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0
[2021-09-28 16:55:33,265] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2021-09-28 16:55:33,265] [INFO] [launch.py:102:main] dist_world_size=1
[2021-09-28 16:55:33,265] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0
using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
oneflow-22:2669950:2669950 [0] NCCL INFO Bootstrap : Using [0]eno1:192.168.1.22<0> [1]veth8a96493:fe80::a405:1cff:fed9:b8eb%veth8a96493<0> [2]veth2939e96:fe80::9c62:fbff:fe29:7ce1%veth2939e96<0>
oneflow-22:2669950:2669950 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
oneflow-22:2669950:2669950 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
oneflow-22:2669950:2669950 [0] NCCL INFO NET/IB : Using [0]iwo1:1/RoCE ; OOB eno1:192.168.1.22<0>
oneflow-22:2669950:2669950 [0] NCCL INFO Using network IB
NCCL version 2.7.8+cuda10.2
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 00/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 01/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 02/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 03/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 04/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 05/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 06/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 07/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 08/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 09/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 10/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 11/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 12/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 13/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 14/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 15/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 16/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 17/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 18/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 19/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 20/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 21/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 22/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 23/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 24/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 25/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 26/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 27/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 28/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 29/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 30/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Channel 31/32 :    0
oneflow-22:2669950:2670298 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1-
oneflow-22:2669950:2670298 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,ffff0000
oneflow-22:2669950:2670298 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
oneflow-22:2669950:2670298 [0] NCCL INFO comm 0x7f7e9c0010a0 rank 0 nranks 1 cudaDev 0 busId af000 - Init COMPLETE
> initializing model parallel with size 1
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
loading BertWordPieceTokenizer ( bert-base-uncased ) from cache_dir  None
loaded bert-base-uncased
> padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> found end-of-document token: 0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 00/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 01/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 02/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 03/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 04/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 05/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 06/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 07/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 08/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 09/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 10/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 11/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 12/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 13/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 14/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 15/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 16/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 17/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 18/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 19/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 20/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 21/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 22/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 23/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 24/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 25/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 26/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 27/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 28/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 29/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 30/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Channel 31/32 :    0
oneflow-22:2669950:2670306 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1-
oneflow-22:2669950:2670306 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,ffff0000
oneflow-22:2669950:2670306 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
oneflow-22:2669950:2670306 [0] NCCL INFO comm 0x7f7ea00010f0 rank 0 nranks 1 cudaDev 0 busId af000 - Init COMPLETE
Creating copa dataset from file at /dataset/lichunyou/GLM/GLM_copa/dataset/COPA (split=test)
Returning 500 test examples with label dist.: [(None, 500)]
Traceback (most recent call last):
  File "finetune_glm.py", line 480, in <module>
    main(args)
  File "/dataset/lichunyou/GLM/GLM_copa/tasks/superglue/finetune.py", line 115, in main
    end_of_epoch_callback_provider=metrics_func_provider
  File "/dataset/lichunyou/GLM/GLM_copa/finetune_glm.py", line 357, in finetune
    model, optimizer, lr_scheduler = setup_model_and_optimizer(args, **model_kwargs)
  File "/dataset/lichunyou/GLM/GLM_copa/train_utils.py", line 270, in setup_model_and_optimizer
    spell_length=spell_length)
  File "/dataset/lichunyou/GLM/GLM_copa/train_utils.py", line 125, in get_model
    attention_scale=args.attention_scale)
  File "/dataset/lichunyou/GLM/GLM_copa/model/modeling_glm.py", line 93, in __init__
    block_position_encoding=block_position_encoding)
  File "/dataset/lichunyou/GLM/GLM_copa/mpu/transformer.py", line 702, in __init__
    [get_layer() for _ in range(num_layers)])
  File "/dataset/lichunyou/GLM/GLM_copa/mpu/transformer.py", line 702, in <listcomp>
    [get_layer() for _ in range(num_layers)])
  File "/dataset/lichunyou/GLM/GLM_copa/mpu/transformer.py", line 698, in get_layer
    attention_scale=attention_scale)
  File "/dataset/lichunyou/GLM/GLM_copa/mpu/transformer.py", line 538, in __init__
    self.input_layernorm = LayerNorm(hidden_size, eps=layernorm_epsilon)
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/site-packages/apex-0.1-py3.7.egg/apex/normalization/fused_layer_norm.py", line 133, in __init__
    fused_layer_norm_cuda = importlib.import_module("fused_layer_norm_cuda")
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'fused_layer_norm_cuda'
Killing subprocess 2669950
Traceback (most recent call last):
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/site-packages/deepspeed/launcher/launch.py", line 171, in <module>
    main()
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/site-packages/deepspeed/launcher/launch.py", line 161, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/dataset/lichunyou/conda_env/miniconda3/envs/p7/lib/python3.7/site-packages/deepspeed/launcher/launch.py", line 139, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/dataset/lichunyou/conda_env/miniconda3/envs/p7/bin/python', '-u', 'finetune_glm.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'config_tasks/config_blocklm_10B.json', '--finetune', '--cloze-eval', '--experiment-name', 'blank-base-copa_09-28-16-55', '--task', 'COPA', '--data-dir', '/dataset/lichunyou/GLM/GLM_copa/dataset/COPA', '--save', '/dataset/lichunyou/GLM/GLM_copa/copa_model', '--seq-length', '256', '--checkpoint-activations', '--eval-batch-size', '4', '--save-epoch', '100000', '--num-workers', '1', '--no-load-optim', '--no-load-lr-scheduler', '--block-lm', '--num-layers', '12', '--hidden-size', '768', '--num-attention-heads', '12', '--max-position-embeddings', '512', '--tokenizer-model-type', 'bert-base-uncased', '--tokenizer-type', 'BertWordPieceTokenizer', '--load-pretrained', '/dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55', '--lr-decay-style', 'linear', '--warmup', '0.1', '--weight-decay', '1.0e-1', '--pattern-id', '0', '--save-interval', '10000', '--log-interval', '20', '--eval-interval', '1000', '--eval-iters', '100', '--pattern-id', '0', '--fp16', '--model-parallel-size', '1', '--epochs', '-1', '--overwrite']' returned non-zero exit status 1.
