[2021-09-30 15:42:57,316] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-09-30 15:42:57,641] [INFO] [runner.py:360:main] cmd = /dataset/lichunyou/conda_env/miniconda3/envs/p7/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=53076 finetune_glm.py --deepspeed --deepspeed_config config_tasks/config_blocklm_10B.json --finetune --cloze-eval --experiment-name blank-base-copa_09-30-15-42 --task COPA --data-dir /dataset/lichunyou/GLM/GLM_copa/dataset/COPA --save /dataset/lichunyou/GLM/GLM_copa/copa_model --seq-length 256 --checkpoint-activations --eval-batch-size 4 --save-epoch 100000 --num-workers 1 --no-load-optim --no-load-lr-scheduler --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --pattern-id 0 --fp16 --model-parallel-size 1 --epochs -1 --overwrite
[2021-09-30 15:42:58,296] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-09-30 15:42:58,296] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-09-30 15:42:58,296] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-09-30 15:42:58,296] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}
[2021-09-30 15:42:58,296] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0
[2021-09-30 15:42:58,296] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2021-09-30 15:42:58,296] [INFO] [launch.py:102:main] dist_world_size=1
[2021-09-30 15:42:58,297] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0
using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
oneflow-22:904576:904576 [0] NCCL INFO Bootstrap : Using [0]eno1:192.168.1.22<0> [1]veth8a96493:fe80::a405:1cff:fed9:b8eb%veth8a96493<0> [2]veth2939e96:fe80::9c62:fbff:fe29:7ce1%veth2939e96<0>
oneflow-22:904576:904576 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
oneflow-22:904576:904576 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
oneflow-22:904576:904576 [0] NCCL INFO NET/IB : Using [0]iwo1:1/RoCE ; OOB eno1:192.168.1.22<0>
oneflow-22:904576:904576 [0] NCCL INFO Using network IB
NCCL version 2.7.8+cuda10.2
oneflow-22:904576:905198 [0] NCCL INFO Channel 00/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 01/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 02/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 03/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 04/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 05/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 06/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 07/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 08/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 09/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 10/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 11/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 12/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 13/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 14/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 15/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 16/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 17/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 18/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 19/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 20/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 21/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 22/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 23/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 24/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 25/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 26/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 27/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 28/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 29/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 30/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Channel 31/32 :    0
oneflow-22:904576:905198 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0
oneflow-22:904576:905198 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,ffff0000
oneflow-22:904576:905198 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
oneflow-22:904576:905198 [0] NCCL INFO comm 0x7f07400010a0 rank 0 nranks 1 cudaDev 0 busId af000 - Init COMPLETE
> initializing model parallel with size 1
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
loading BertWordPieceTokenizer ( bert-base-uncased ) from cache_dir  None
loaded bert-base-uncased
> padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> found end-of-document token: 0
oneflow-22:904576:905209 [0] NCCL INFO Channel 00/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 01/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 02/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 03/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 04/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 05/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 06/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 07/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 08/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 09/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 10/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 11/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 12/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 13/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 14/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 15/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 16/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 17/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 18/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 19/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 20/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 21/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 22/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 23/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 24/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 25/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 26/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 27/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 28/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 29/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 30/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Channel 31/32 :    0
oneflow-22:904576:905209 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0
oneflow-22:904576:905209 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,ffff0000
oneflow-22:904576:905209 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
oneflow-22:904576:905209 [0] NCCL INFO comm 0x7f07480010f0 rank 0 nranks 1 cudaDev 0 busId af000 - Init COMPLETE
Creating copa dataset from file at /dataset/lichunyou/GLM/GLM_copa/dataset/COPA (split=test)
Returning 500 test examples with label dist.: [(None, 500)]
 > number of parameters on model parallel rank 0: 109338624
global rank 0 is loading pretrained model /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55/best/mp_rank_00_model_states.pt
arguments:
  transformer_xl ............... False
  pretrained_bert .............. False
  encoder_decoder .............. False
  attention_dropout ............ 0.1
  num_attention_heads .......... 12
  hidden_size .................. 768
  intermediate_size ............ None
  num_layers ................... 12
  layernorm_epsilon ............ 1e-05
  hidden_dropout ............... 0.1
  output_dropout ............... 0.1
  max_position_embeddings ...... 512
  vocab_size ................... 30592
  deep_init .................... False
  make_vocab_size_divisible_by . 128
  cpu_optimizer ................ False
  cpu_torch_adam ............... False
  fp16 ......................... True
  fp32_embedding ............... False
  fp32_layernorm ............... False
  fp32_tokentypes .............. False
  fp32_allreduce ............... False
  hysteresis ................... 2
  loss_scale ................... None
  loss_scale_window ............ 1000
  min_scale .................... 1
  attention_scale .............. 1.0
  experiment_name .............. blank-base-copa_09-30-15-42
  batch_size ................... 4
  gradient_accumulation_steps .. 2
  weight_decay ................. 0.01
  checkpoint_activations ....... True
  checkpoint_num_layers ........ 1
  deepspeed_activation_checkpointing  False
  epochs ....................... -1
  clip_grad .................... 1.0
  train_iters .................. 0
  label_smoothing .............. 0.0
  log_interval ................. 20
  summary_dir .................. 
  seed ......................... 1234
  reset_position_ids ........... False
  reset_attention_mask ......... False
  lr_decay_iters ............... None
  lr_decay_style ............... linear
  lr_decay_ratio ............... 0.1
  lr ........................... 5e-06
  warmup ....................... 0.1
  switch_linear ................ False
  save ......................... /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_09-30-15-42
  new_save_directory ........... False
  save_epoch ................... 100000
  save_interval ................ 10000
  no_save_optim ................ False
  no_save_rng .................. False
  load ......................... None
  no_load_optim ................ True
  no_load_rng .................. False
  no_load_lr_scheduler ......... True
  no_deepspeed_load ............ False
  finetune ..................... True
  resume_dataloader ............ False
  distributed_backend .......... nccl
  DDP_impl ..................... torch
  local_rank ................... 0
  block_lm ..................... True
  masked_lm .................... False
  bert_prob .................... 0.5
  gpt_infill_prob .............. 0.5
  gpt_min_ratio ................ 0.5
  gap_sentence_prob ............ 0.0
  gap_sentence_ratio ........... 0.15
  avg_block_length ............. 3
  short_seq_prob ............... 0.0
  single_span_prob ............. 0.0
  task_mask .................... False
  no_shuffle_block ............. False
  no_block_position ............ False
  sentinel_token ............... False
  block_mask_prob .............. 0.0
  context_mask_ratio ........... 0.0
  random_position .............. False
  eval_batch_size .............. 4
  eval_iters ................... 100
  eval_interval ................ 1000
  eval_epoch ................... 1
  eval_seq_length .............. None
  eval_max_preds_per_seq ....... None
  overlapping_eval ............. 32
  temperature .................. 1.0
  top_p ........................ 0.0
  top_k ........................ 0
  out_seq_length ............... 256
  num_beams .................... 1
  length_penalty ............... 0.0
  no_repeat_ngram_size ......... 0
  min_tgt_length ............... 0
  select_topk .................. False
  blank_maskratio .............. 0.1
  model_parallel_size .......... 1
  shuffle ...................... False
  filter_english ............... False
  train_data ................... None
  valid_data ................... None
  test_data .................... None
  data_dir ..................... /dataset/lichunyou/GLM/GLM_copa/dataset/COPA
  input_data_sizes_file ........ sizes.txt
  delim ........................ ,
  text_key ..................... sentence
  eval_text_key ................ None
  split ........................ 1000,1,1
  no_lazy_loader ............... False
  half_lazy_loader ............. False
  loader_scatter ............... None
  loose_json ................... False
  presplit_sentences ........... False
  num_workers .................. 1
  tokenizer_model_type ......... bert-base-uncased
  tokenizer_path ............... tokenizer.model
  tokenizer_type ............... BertWordPieceTokenizer
  no_pre_tokenize .............. False
  cache_dir .................... None
  use_tfrecords ................ False
  seq_length ................... 256
  mem_length ................... 0
  max_preds_per_seq ............ None
  non_sentence_start ........... 0.0
  sample_one_document .......... False
  load_splits .................. None
  save_splits .................. None
  save_test_data ............... None
  multi_task_data .............. None
  multi_task_ratio ............. 0.0
  multi_seq_length ............. None
  multi_batch_size ............. None
  task ......................... COPA
  load_pretrained .............. /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55
  pool_token ................... cls
  cloze_eval ................... True
  multi_token .................. False
  segment_length ............... 0
  loss_func .................... cross_entropy
  block_lm_ratio ............... 0.0
  adapet ....................... False
  pattern_id ................... 0
  fast_decode .................. False
  few_superglue ................ False
  eval_valid ................... False
  validation_metric ............ None
  unidirectional ............... False
  src_seq_length ............... None
  tgt_seq_length ............... None
  adam_beta1 ................... 0.9
  adam_beta2 ................... 0.999
  adam_eps ..................... 1e-08
  optimizer .................... adam
  wsc_negative ................. False
  overwrite .................... True
  no_validation ................ False
  continuous_prompt ............ False
  num_prompt_tokens ............ 0
  prompt_func .................. lstm
  freeze_transformer ........... False
  tune_prefix_layers ........... None
  prefix_prompt ................ 0
  prompt_init .................. False
  deepspeed .................... True
  deepspeed_config ............. config_tasks/config_blocklm_10B.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  cuda ......................... True
  rank ......................... 0
  world_size ................... 1
  dynamic_loss_scale ........... True
  master_ip .................... 127.0.0.1
  master_port .................. 53076
  eod_token .................... 0
  variable_num_choices ......... False
  iteration .................... 0
  log_dir ...................... runs/blank-base-copa_09-30-15-42
done with setups ...
time (ms) | train/valid/test dataset/dataloder: 0.01 | callback function: 5.64 | model and optimizer: 1462.85 | pretrained checkpoint: 231.62
training ...
evaluation only mode, setting epoch to -1
Using port 63803
Partition Activations False and Correctness Check False
(tensor([[[ 1.8096,  1.7979,  2.9258,  ...,  2.6836,  1.6484,  0.5361],
         [ 0.0468,  1.4355,  1.0127,  ...,  3.8340, -1.1133, -1.5156],
         [ 0.6123, -0.5522,  1.9736,  ...,  2.9102,  2.9336, -2.1289],
         ...,
         [ 0.9487,  0.7002,  1.1533,  ...,  1.2432,  3.0039,  0.6118],
         [ 0.9531,  0.6987,  1.1523,  ...,  1.2432,  3.0078,  0.6152],
         [ 0.9512,  0.7002,  1.1553,  ...,  1.2422,  3.0039,  0.6147]],

        [[ 1.8096,  1.7979,  2.9258,  ...,  2.6836,  1.6484,  0.5361],
         [ 0.0468,  1.4355,  1.0127,  ...,  3.8340, -1.1133, -1.5156],
         [ 0.6123, -0.5522,  1.9736,  ...,  2.9102,  2.9336, -2.1289],
         ...,
         [ 0.9351,  0.7197,  1.1338,  ...,  1.2451,  3.0117,  0.5879],
         [ 0.9351,  0.7183,  1.1338,  ...,  1.2432,  3.0156,  0.5879],
         [ 0.9346,  0.7139,  1.1338,  ...,  1.2441,  3.0137,  0.5898]],

        [[ 1.9932,  2.7656,  1.0381,  ...,  1.7100,  0.7153,  1.7480],
         [-0.1901,  1.8174, -1.9209,  ...,  3.0137, -0.1133,  0.4939],
         [ 1.9805, -0.3557, -0.6196,  ...,  1.3662,  0.3142,  2.1113],
         ...,
         [ 1.2539,  0.9688,  1.2764,  ...,  0.9648,  2.7129,  1.1592],
         [ 1.2549,  0.9624,  1.2754,  ...,  0.9653,  2.7148,  1.1582],
         [ 1.2520,  0.9644,  1.2773,  ...,  0.9653,  2.7129,  1.1562]],

        ...,

        [[-0.0519,  1.7793,  1.3867,  ...,  2.0664, -2.0215, -0.1490],
         [ 0.0140,  1.8164, -1.1289,  ...,  2.3145, -0.7393,  0.4453],
         [-1.7432,  2.8594,  0.3826,  ...,  1.0908,  0.8716, -0.8027],
         ...,
         [ 1.1016,  0.6826,  1.3242,  ...,  1.0928,  2.5840,  0.9551],
         [ 1.0996,  0.6870,  1.3252,  ...,  1.0928,  2.5879,  0.9541],
         [ 1.0986,  0.6870,  1.3242,  ...,  1.0938,  2.5820,  0.9526]],

        [[ 1.3994,  1.1475,  2.7656,  ...,  3.4668,  0.8081,  0.5190],
         [ 2.4766, -0.1774, -0.2334,  ...,  4.7695, -0.5747, -0.1842],
         [ 0.1056, -1.3652, -0.3306,  ...,  2.7383,  0.1394,  0.6006],
         ...,
         [ 0.9517,  1.1113,  1.6621,  ...,  1.2822,  2.7227,  0.6626],
         [ 0.9521,  1.1104,  1.6680,  ...,  1.2832,  2.7266,  0.6631],
         [ 0.9536,  1.1055,  1.6660,  ...,  1.2842,  2.7324,  0.6646]],

        [[ 1.3994,  1.1475,  2.7656,  ...,  3.4668,  0.8081,  0.5190],
         [ 2.4766, -0.1774, -0.2334,  ...,  4.7695, -0.5747, -0.1842],
         [ 0.1056, -1.3652, -0.3306,  ...,  2.7383,  0.1394,  0.6006],
         ...,
         [ 0.9658,  1.1143,  1.6768,  ...,  1.2852,  2.7070,  0.6812],
         [ 0.9658,  1.1113,  1.6777,  ...,  1.2881,  2.7051,  0.6836],
         [ 0.9639,  1.1123,  1.6787,  ...,  1.2871,  2.7051,  0.6831]]],
       device='cuda:0', dtype=torch.float16), [])
