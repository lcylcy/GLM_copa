[2021-10-08 11:41:59,413] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2021-10-08 11:42:01,249] [INFO] [runner.py:360:main] cmd = /dataset/lichunyou/conda_env/miniconda3/envs/p7/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=15577 finetune_glm.py --deepspeed --deepspeed_config config_tasks/config_blocklm_10B.json --finetune --cloze-eval --experiment-name blank-base-copa_10-08-11-41 --task COPA --data-dir /dataset/lichunyou/GLM/GLM_copa/dataset/COPA --save /dataset/lichunyou/GLM/GLM_copa/copa_model --seq-length 256 --checkpoint-activations --eval-batch-size 4 --save-epoch 100000 --num-workers 1 --no-load-optim --no-load-lr-scheduler --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --pattern-id 0 --fp16 --model-parallel-size 1 --epochs -1 --overwrite
[2021-10-08 11:42:02,009] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 0
[2021-10-08 11:42:02,009] [INFO] [launch.py:73:main] 0 NCCL_DEBUG info
[2021-10-08 11:42:02,009] [INFO] [launch.py:73:main] 0 NCCL_NET_GDR_LEVEL 2
[2021-10-08 11:42:02,009] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}
[2021-10-08 11:42:02,009] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0
[2021-10-08 11:42:02,009] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2021-10-08 11:42:02,009] [INFO] [launch.py:102:main] dist_world_size=1
[2021-10-08 11:42:02,009] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0
using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
oneflow-22:872161:872161 [0] NCCL INFO Bootstrap : Using [0]eno1:192.168.1.22<0> [1]veth8a96493:fe80::a405:1cff:fed9:b8eb%veth8a96493<0> [2]veth2939e96:fe80::9c62:fbff:fe29:7ce1%veth2939e96<0>
oneflow-22:872161:872161 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
oneflow-22:872161:872161 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
oneflow-22:872161:872161 [0] NCCL INFO NET/IB : Using [0]iwo1:1/RoCE ; OOB eno1:192.168.1.22<0>
oneflow-22:872161:872161 [0] NCCL INFO Using network IB
NCCL version 2.7.8+cuda10.2
oneflow-22:872161:872419 [0] NCCL INFO Channel 00/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 01/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 02/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 03/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 04/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 05/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 06/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 07/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 08/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 09/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 10/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 11/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 12/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 13/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 14/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 15/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 16/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 17/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 18/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 19/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 20/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 21/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 22/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 23/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 24/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 25/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 26/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 27/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 28/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 29/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 30/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Channel 31/32 :    0
oneflow-22:872161:872419 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0
oneflow-22:872161:872419 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,ffff0000
oneflow-22:872161:872419 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
oneflow-22:872161:872419 [0] NCCL INFO comm 0x7fee580010a0 rank 0 nranks 1 cudaDev 0 busId af000 - Init COMPLETE
> initializing model parallel with size 1
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
loading BertWordPieceTokenizer ( bert-base-uncased ) from cache_dir  None
loaded bert-base-uncased
> padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> found end-of-document token: 0
oneflow-22:872161:872424 [0] NCCL INFO Channel 00/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 01/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 02/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 03/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 04/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 05/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 06/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 07/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 08/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 09/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 10/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 11/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 12/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 13/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 14/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 15/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 16/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 17/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 18/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 19/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 20/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 21/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 22/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 23/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 24/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 25/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 26/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 27/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 28/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 29/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 30/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Channel 31/32 :    0
oneflow-22:872161:872424 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0
oneflow-22:872161:872424 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,ffff0000
oneflow-22:872161:872424 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
oneflow-22:872161:872424 [0] NCCL INFO comm 0x7fee5c0010f0 rank 0 nranks 1 cudaDev 0 busId af000 - Init COMPLETE
Creating copa dataset from file at /dataset/lichunyou/GLM/GLM_copa/dataset/COPA (split=test)
Returning 500 test examples with label dist.: [(None, 500)]
 > number of parameters on model parallel rank 0: 109338624
global rank 0 is loading pretrained model /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55/best/mp_rank_00_model_states.pt
arguments:
  transformer_xl ............... False
  pretrained_bert .............. False
  encoder_decoder .............. False
  attention_dropout ............ 0.1
  num_attention_heads .......... 12
  hidden_size .................. 768
  intermediate_size ............ None
  num_layers ................... 12
  layernorm_epsilon ............ 1e-05
  hidden_dropout ............... 0.1
  output_dropout ............... 0.1
  max_position_embeddings ...... 512
  vocab_size ................... 30592
  deep_init .................... False
  make_vocab_size_divisible_by . 128
  cpu_optimizer ................ False
  cpu_torch_adam ............... False
  fp16 ......................... True
  fp32_embedding ............... False
  fp32_layernorm ............... False
  fp32_tokentypes .............. False
  fp32_allreduce ............... False
  hysteresis ................... 2
  loss_scale ................... None
  loss_scale_window ............ 1000
  min_scale .................... 1
  attention_scale .............. 1.0
  experiment_name .............. blank-base-copa_10-08-11-41
  batch_size ................... 4
  gradient_accumulation_steps .. 2
  weight_decay ................. 0.01
  checkpoint_activations ....... True
  checkpoint_num_layers ........ 1
  deepspeed_activation_checkpointing  False
  epochs ....................... -1
  clip_grad .................... 1.0
  train_iters .................. 0
  label_smoothing .............. 0.0
  log_interval ................. 20
  summary_dir .................. 
  seed ......................... 1234
  reset_position_ids ........... False
  reset_attention_mask ......... False
  lr_decay_iters ............... None
  lr_decay_style ............... linear
  lr_decay_ratio ............... 0.1
  lr ........................... 5e-06
  warmup ....................... 0.1
  switch_linear ................ False
  save ......................... /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_10-08-11-41
  new_save_directory ........... False
  save_epoch ................... 100000
  save_interval ................ 10000
  no_save_optim ................ False
  no_save_rng .................. False
  load ......................... None
  no_load_optim ................ True
  no_load_rng .................. False
  no_load_lr_scheduler ......... True
  no_deepspeed_load ............ False
  finetune ..................... True
  resume_dataloader ............ False
  distributed_backend .......... nccl
  DDP_impl ..................... torch
  local_rank ................... 0
  block_lm ..................... True
  masked_lm .................... False
  bert_prob .................... 0.5
  gpt_infill_prob .............. 0.5
  gpt_min_ratio ................ 0.5
  gap_sentence_prob ............ 0.0
  gap_sentence_ratio ........... 0.15
  avg_block_length ............. 3
  short_seq_prob ............... 0.0
  single_span_prob ............. 0.0
  task_mask .................... False
  no_shuffle_block ............. False
  no_block_position ............ False
  sentinel_token ............... False
  block_mask_prob .............. 0.0
  context_mask_ratio ........... 0.0
  random_position .............. False
  eval_batch_size .............. 4
  eval_iters ................... 100
  eval_interval ................ 1000
  eval_epoch ................... 1
  eval_seq_length .............. None
  eval_max_preds_per_seq ....... None
  overlapping_eval ............. 32
  temperature .................. 1.0
  top_p ........................ 0.0
  top_k ........................ 0
  out_seq_length ............... 256
  num_beams .................... 1
  length_penalty ............... 0.0
  no_repeat_ngram_size ......... 0
  min_tgt_length ............... 0
  select_topk .................. False
  blank_maskratio .............. 0.1
  model_parallel_size .......... 1
  shuffle ...................... False
  filter_english ............... False
  train_data ................... None
  valid_data ................... None
  test_data .................... None
  data_dir ..................... /dataset/lichunyou/GLM/GLM_copa/dataset/COPA
  input_data_sizes_file ........ sizes.txt
  delim ........................ ,
  text_key ..................... sentence
  eval_text_key ................ None
  split ........................ 1000,1,1
  no_lazy_loader ............... False
  half_lazy_loader ............. False
  loader_scatter ............... None
  loose_json ................... False
  presplit_sentences ........... False
  num_workers .................. 1
  tokenizer_model_type ......... bert-base-uncased
  tokenizer_path ............... tokenizer.model
  tokenizer_type ............... BertWordPieceTokenizer
  no_pre_tokenize .............. False
  cache_dir .................... None
  use_tfrecords ................ False
  seq_length ................... 256
  mem_length ................... 0
  max_preds_per_seq ............ None
  non_sentence_start ........... 0.0
  sample_one_document .......... False
  load_splits .................. None
  save_splits .................. None
  save_test_data ............... None
  multi_task_data .............. None
  multi_task_ratio ............. 0.0
  multi_seq_length ............. None
  multi_batch_size ............. None
  task ......................... COPA
  load_pretrained .............. /dataset/lichunyou/GLM/GLM_copa/copa_model/blank-base-copa_08-25-23-55
  pool_token ................... cls
  cloze_eval ................... True
  multi_token .................. False
  segment_length ............... 0
  loss_func .................... cross_entropy
  block_lm_ratio ............... 0.0
  adapet ....................... False
  pattern_id ................... 0
  fast_decode .................. False
  few_superglue ................ False
  eval_valid ................... False
  validation_metric ............ None
  unidirectional ............... False
  src_seq_length ............... None
  tgt_seq_length ............... None
  adam_beta1 ................... 0.9
  adam_beta2 ................... 0.999
  adam_eps ..................... 1e-08
  optimizer .................... adam
  wsc_negative ................. False
  overwrite .................... True
  no_validation ................ False
  continuous_prompt ............ False
  num_prompt_tokens ............ 0
  prompt_func .................. lstm
  freeze_transformer ........... False
  tune_prefix_layers ........... None
  prefix_prompt ................ 0
  prompt_init .................. False
  deepspeed .................... True
  deepspeed_config ............. config_tasks/config_blocklm_10B.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  cuda ......................... True
  rank ......................... 0
  world_size ................... 1
  dynamic_loss_scale ........... True
  master_ip .................... 127.0.0.1
  master_port .................. 15577
  eod_token .................... 0
  variable_num_choices ......... False
  iteration .................... 0
  log_dir ...................... runs/blank-base-copa_10-08-11-41
done with setups ...
time (ms) | train/valid/test dataset/dataloder: 0.01 | callback function: 5.40 | model and optimizer: 1432.93 | pretrained checkpoint: 230.36
training ...
evaluation only mode, setting epoch to -1
Using port 57216
Partition Activations False and Correctness Check False
13.04102873802185
